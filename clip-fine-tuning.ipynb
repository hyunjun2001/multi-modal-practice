{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CLIP 모델에 간단한 fine tuning을 적용해 모델을 사용해보자.\nhttps://devs0n.tistory.com/195 해당 글을 참고했습니다.","metadata":{}},{"cell_type":"code","source":"%config Completer.use_jedi = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:31:13.058274Z","iopub.execute_input":"2024-12-17T15:31:13.058619Z","iopub.status.idle":"2024-12-17T15:31:13.083296Z","shell.execute_reply.started":"2024-12-17T15:31:13.058586Z","shell.execute_reply":"2024-12-17T15:31:13.082465Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# 모델 불러오기\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom torch.optim import AdamW\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\noptimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:31:15.219727Z","iopub.execute_input":"2024-12-17T15:31:15.220568Z","iopub.status.idle":"2024-12-17T15:31:41.441254Z","shell.execute_reply.started":"2024-12-17T15:31:15.220523Z","shell.execute_reply":"2024-12-17T15:31:41.440331Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37bf45dfeca44b9796cd565c8c2fa449"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d8f4de797d4d6885802e7b6cb4f05a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807aeb99d29e42a298e29f61da8898ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d2daf594f74bb79d4a9890ce2dd196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791781ec656f4ac0af9adfc46fb18a81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9407e58081244d2ebc11b46d412e1b1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57312fe536164039b5912678d8d9708e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7d64e54a52a4c34a4c49b1f405d7405"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a9180c924ce4195babf0223d510617c"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import torch, torchvision\n\nclass CIFAR10Dataset(torch.utils.data.Dataset):\n    def __init__(self, clip_processor, is_train):\n        self.clip_processor = clip_processor\n        self.is_train = is_train\n        self.dataset = torchvision.datasets.CIFAR10('/kaggle/working',download=True,train=is_train)\n        self.class_texts = [\n            f\"this is {class_}.\"\n            for class_ in self.dataset.class_to_idx.keys()\n        ]\n        \n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, label = self.dataset[idx]\n        text = self.class_texts[label]\n        return {\n            \"image\": image,\n            \"label\": label,\n            \"text\": text,\n        }\n    \n    def preprocess(self, batch):\n        images = [data[\"image\"] for data in batch]\n        labels = [data[\"label\"] for data in batch]\n        texts = [data[\"text\"] for data in batch]\n\n        inputs = self.clip_processor(\n            text=texts,\n            images=images,\n            return_tensors=\"pt\", \n            padding=True\n        )\n        \n        return {\n            \"text\": texts,\n            \"label\": torch.tensor(labels),\n            **inputs,\n        }\n\nbatch_size = 256\n\ntrain_dataset = CIFAR10Dataset(processor, is_train=True)\ntest_dataset = CIFAR10Dataset(processor, is_train=False)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,\n                                               collate_fn=train_dataset.preprocess, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset,\n                                              collate_fn=train_dataset.preprocess, batch_size=batch_size)\n\nprint(len(train_dataset))\nprint(len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:31:44.115032Z","iopub.execute_input":"2024-12-17T15:31:44.115811Z","iopub.status.idle":"2024-12-17T15:31:59.800192Z","shell.execute_reply.started":"2024-12-17T15:31:44.115754Z","shell.execute_reply":"2024-12-17T15:31:59.799287Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /kaggle/working/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:10<00:00, 15734291.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /kaggle/working/cifar-10-python.tar.gz to /kaggle/working\nFiles already downloaded and verified\n50000\n10000\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef loss_fn(logits_per_image, logits_per_text):\n    assert logits_per_image.shape[0] == logits_per_image.shape[0] # logits' shape should be (nxn)\n    assert logits_per_image.shape == logits_per_text.shape\n    \n    labels = torch.arange(logits_per_image.shape[0], device=device)\n    loss_i = F.cross_entropy(logits_per_image, labels)\n    loss_t = F.cross_entropy(logits_per_text, labels)\n    loss = (loss_i + loss_t) / 2\n    \n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:32:03.313796Z","iopub.execute_input":"2024-12-17T15:32:03.314282Z","iopub.status.idle":"2024-12-17T15:32:03.323677Z","shell.execute_reply.started":"2024-12-17T15:32:03.314227Z","shell.execute_reply":"2024-12-17T15:32:03.320414Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train(model, dataloader, optimizer):\n    for batch in tqdm(dataloader, position=0, desc=\"batch\", leave=False):\n        optimizer.zero_grad()\n\n        outputs = model(\n            pixel_values=batch[\"pixel_values\"].to(device),\n            input_ids=batch[\"input_ids\"].to(device),\n            attention_mask=batch[\"attention_mask\"].to(device),\n        )\n\n        logits_per_image = outputs.logits_per_image\n        logits_per_text = outputs.logits_per_text # logits_per_text == logits_per_image.T\n        loss = loss_fn(logits_per_image, logits_per_text)\n        loss.backward()\n                \n        optimizer.step()\n\n    print(loss)\n\nepochs = 5\n\nfor epoch in range(epochs):\n    train(model, train_dataloader, optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:32:04.234563Z","iopub.execute_input":"2024-12-17T15:32:04.235674Z","iopub.status.idle":"2024-12-17T15:52:11.429486Z","shell.execute_reply.started":"2024-12-17T15:32:04.235618Z","shell.execute_reply":"2024-12-17T15:52:11.428604Z"}},"outputs":[{"name":"stderr","text":"                                                        \r","output_type":"stream"},{"name":"stdout","text":"tensor(2.6383, device='cuda:0', grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"                                                        \r","output_type":"stream"},{"name":"stdout","text":"tensor(2.4036, device='cuda:0', grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"                                                        \r","output_type":"stream"},{"name":"stdout","text":"tensor(2.2878, device='cuda:0', grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"                                                        \r","output_type":"stream"},{"name":"stdout","text":"tensor(2.2076, device='cuda:0', grad_fn=<DivBackward0>)\n","output_type":"stream"},{"name":"stderr","text":"                                                        \r","output_type":"stream"},{"name":"stdout","text":"tensor(2.2722, device='cuda:0', grad_fn=<DivBackward0>)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch.nn.functional as F\n\nall_class_texts = processor.tokenizer(test_dataset.class_texts)\nall_class_texts = {k: torch.tensor(v, device=device) for k, v in all_class_texts.items()}\n\nmodel.eval()\ncorrect_count = 0\nce_loss_sum = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        outputs = model(\n            pixel_values=batch[\"pixel_values\"].to(device),\n            **all_class_texts,\n        )\n        \n        probs = outputs.logits_per_image.cpu().softmax(dim=1)\n        pred = probs.argmax(dim=1)\n        label = batch[\"label\"]\n\n        correct_count += (pred == label).sum().item()\n        ce_loss_sum += F.cross_entropy(probs, label).item()\n    \naccuracy = correct_count / len(test_dataloader.dataset)\nce_loss = ce_loss_sum / len(test_dataloader)\nprint(f\"Test cross entropy loss: {ce_loss:.4}, Test accuracy: {accuracy:.4}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T15:59:13.327360Z","iopub.execute_input":"2024-12-17T15:59:13.328166Z","iopub.status.idle":"2024-12-17T15:59:51.067673Z","shell.execute_reply.started":"2024-12-17T15:59:13.328134Z","shell.execute_reply":"2024-12-17T15:59:51.066723Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 40/40 [00:37<00:00,  1.06it/s]","output_type":"stream"},{"name":"stdout","text":"Test cross entropy loss: 1.53, Test accuracy: 0.94\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}